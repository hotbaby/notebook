{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "maritime-calendar",
   "metadata": {},
   "source": [
    "# Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-reduction",
   "metadata": {},
   "source": [
    "## Kafka相关概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-sentence",
   "metadata": {},
   "source": [
    "Kakfa定位为一个分布式流式处理平台，高吞吐、可持久化、可水平扩展、支持流式数据处理。kafka可以被用作：\n",
    "\n",
    "* 消息系统，实现系统解耦、冗余存储、流量削峰，kafka还提供了大多数消息系统难以实现的**消息顺序性保障**和**消息回溯**功能\n",
    "* 存储系统，kafka把消息持久化到磁盘\n",
    "* 流式处理平台"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-champagne",
   "metadata": {},
   "source": [
    "### kafka技术架构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-teaching",
   "metadata": {},
   "source": [
    "![](https://bj.bcebos.com/ipic/Kafka体系架构.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-machinery",
   "metadata": {},
   "source": [
    "* Producer生产者，发送消息的一方，负责把消息发送到kafka中\n",
    "* Consumer消费者，接收消息的一方，消费者连接到kafka上接收消息，并处理相应的业务逻辑\n",
    "* Broker服务代理节点，Broker可以看做一个Kakfa服务节点或者实例。\n",
    "* ZooKeeper，管理集群的元数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-stack",
   "metadata": {},
   "source": [
    "### 主题和分区"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-rainbow",
   "metadata": {},
   "source": [
    "主题是一个逻辑上的概念，它还可以细分多个分区，一个分区只属于单个主题。同一主题下的不同分区包含的消息式不同的，分区在存储层面可以看作一个可追加的日志（Log）文件，消息在被追加到分区日志文件时候都会分配一个特定的偏移量（offset）。offset是消息在分区中的唯一标识，**kafka通过offset保证分区内的顺序性，不过offset并不跨分区，kafka保证的式分区有序而不是主题有序**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-monroe",
   "metadata": {},
   "source": [
    "### 副本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-horror",
   "metadata": {},
   "source": [
    "kafka为分区引入了多副本（Replic）机制，通过增加副本数量可以提升容灾能力，副本之间式\"一主多从\"的关系，其中leader副本负责处理读写请求，follower副本只负责与leader副本的消息同步。副本处于不同的broker中，当leader副本发生故障时，从follower副本重新选举leader副本对外提供服务。Kafka通过多副本机制实现了故障的自动转移，当kafka某个broker失效时仍然能保证服务可用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-domain",
   "metadata": {},
   "source": [
    "分区中所有的副本统称为AR（Assigned Replicas），所有与leader副本保持一定程度同步的副本组成ISR（In-Sync Replicas），与leader副本滞后过多的副本组成OSR（Out-Of-Sync Replicas）。 `AR = ISR + OSR`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-campaign",
   "metadata": {},
   "source": [
    "leader副本故障时，只有ISR的副本才有资格被选举成leader副本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-canvas",
   "metadata": {},
   "source": [
    "![kafka分区偏移量说明](https://bj.bcebos.com/ipic/Kafka分区中偏移量说明.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-journal",
   "metadata": {},
   "source": [
    "ISR与HW和LEO有紧密的关系。HW是High Watermark的缩写，俗称高水位，它标识了一个特殊的偏移量（offset），消费者只能拉取这个offset之前的消息。LEO是Log End Offset的缩写，它标识当前日志文件中下一条待写入消息的偏移offset，LEO的大小等于当前日志分区中最后一条消息的offset加1。分区ISR集合中的每个副本都会维护自己的LEO，ISR集合中最小LEO为分区的HW，对于消费者而言只能消费HW之前的消息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-syracuse",
   "metadata": {},
   "source": [
    "kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。同步复制要求所有的follower副本都复制完，这条消息才会被确认提交，这种方式极大的影响了性能。异步复制，follower副本异步地从leader副本中复制数据，数据只被leader副本写入就被认为已经成功提交。在这种情况下，如果follower副本复制都落后于leader副本，突然leader副本宕机，则会造成数据丢失。kafka使用这种ISR的方式有效地权衡了数据可靠性和性能之间的关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-microphone",
   "metadata": {},
   "source": [
    "## 生产者"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-uganda",
   "metadata": {},
   "source": [
    "### 重要的生产者参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-blackberry",
   "metadata": {},
   "source": [
    "* `acks`这个参数用来指定分区中必须要有多少个副本收到这条消息，之后生产者才会认为这条消息是成功写入的。\n",
    "    * `acks=1`，默认值为1，生产者发送消息之后，只要分区中的leader副本写入消息，那么就会收到服务端的成功响应\n",
    "    * `acks=0`，生产者发送消息之后，不需要等待任何服务端的响应\n",
    "    * `acks=-1`或者`acks=all`，生产者在发送消息成功后，需要等待ISR中的所有副本都成功写入消息后，才能收到服务端的响应"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-apartment",
   "metadata": {},
   "source": [
    "## 消费者"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-compound",
   "metadata": {},
   "source": [
    "### 消费者和消费者组"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-liability",
   "metadata": {},
   "source": [
    "消费者（Consumer）负责订阅kafka中的主题（Topic），并且从订阅的主题上拉取消息。在kafka中，还有消费者组（Consumer Group）的概念，每个消费者都有一个对应的消费者组，当消费者发布到主题后，只会被投递到它的每个消费者组中的一个消费者。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-ranch",
   "metadata": {},
   "source": [
    "![](https://bj.bcebos.com/ipic/Kafka消费者与消费者组.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-queen",
   "metadata": {},
   "source": [
    "消费者组是一个逻辑上的概念，它将旗下的消费者归为一类，每个消费者只隶属于一个消费者组。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-dealing",
   "metadata": {},
   "source": [
    "### 客户端开发"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-appreciation",
   "metadata": {},
   "source": [
    "一个正常的消费逻辑包括以下几个步骤：\n",
    "\n",
    "1. 配置客户端参数，并创建消费者实例\n",
    "2. 订阅主题\n",
    "3. 拉取消息并消费\n",
    "4. 提交消费位移\n",
    "5. 关闭消费者实例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-cycle",
   "metadata": {},
   "source": [
    "### 位移提交"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-startup",
   "metadata": {},
   "source": [
    "消费者每次调用`poll()`方法时，它返回的是还没有消费过的消息。要做到这一点就必须记录上次消费的消费位移，并且这个消费位移要持久化保存，而不是保存在内存中，否则消费者重启之后就无法知道之前的消费位移。当有新的消费者加入时，必然会有再均衡的操作，对于一个分区而言，它可能在再均衡操作之后分配新的消费者，如果不持久化保存消费位移，那么新的消费者就无法知道之前的消费位移。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-fighter",
   "metadata": {},
   "source": [
    "#### 重复消费和消息丢失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-victorian",
   "metadata": {},
   "source": [
    "* 重复消费，消息被消费，位移未提交\n",
    "* 消息丢失，位移已提交，消息未消费"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-dairy",
   "metadata": {},
   "source": [
    "kafka默认的位移提交方式为自动提交，这个由消费者的客户端参数`enable.auto.commit`配置，默认为`true`。消费者每隔5秒会拉取分区中最大的消费位移进行提交，自动提交位移的动作是再`poll()`方法的逻辑里实现的。**在每次向服务器发起拉取请求之前，会检查是否有新的位移需要提交，如果可以，会提交上一次轮询的位移。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-blend",
   "metadata": {},
   "source": [
    "### 再均衡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-command",
   "metadata": {},
   "source": [
    "再均衡是指分区的所属权从一个消费者转移到另一消费者的行为，它为消费者具备高可用和伸缩性提供了保障，是我们可以方便安全的删除消费者组内的消费者或者往消费者组内添加消费者。**再均衡期间，消费组内的消费者是无法读取消息的。也就是说，再均衡期间，消费者变得不可用。** 另外，当一个分区被重新分配给另一个消费者时，消费者的当前状态也会消失。比如消费者消费完某一个分区的一部分消息时还没有来得及提交消费位移就发生了再均衡操作，之后这个分区又分配给消费者组内另外一个消费者，原来被消费完的消息又被重新消费了一遍，也就是发生了**重复消费**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-flight",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "class ConsumerCoordinator(BaseCoordinator):\n",
    "    \"\"\"This class manages the coordination process with the consumer coordinator.\"\"\"\n",
    "\n",
    "\n",
    "    def _on_join_prepare(self, generation, member_id):\n",
    "        # 如果开启了自动位移提交，再均衡时会自动提交当前的消费位移。\n",
    "        # commit offsets prior to rebalance if auto-commit enabled\n",
    "        self._maybe_auto_commit_offsets_sync()\n",
    "\n",
    "        # execute the user's callback before rebalance\n",
    "        log.info(\"Revoking previously assigned partitions %s for group %s\",\n",
    "                 self._subscription.assigned_partitions(), self.group_id)\n",
    "        if self._subscription.listener:\n",
    "            try:\n",
    "                revoked = set(self._subscription.assigned_partitions())\n",
    "                self._subscription.listener.on_partitions_revoked(revoked)\n",
    "            except Exception:\n",
    "                log.exception(\"User provided subscription listener %s\"\n",
    "                              \" for group %s failed on_partitions_revoked\",\n",
    "                              self._subscription.listener, self.group_id)\n",
    "\n",
    "        self._is_leader = False\n",
    "        self._subscription.reset_group_subscription()\n",
    "\n",
    "    def _maybe_auto_commit_offsets_sync(self):\n",
    "        if self.config['enable_auto_commit']:\n",
    "            try:\n",
    "                self.commit_offsets_sync(self._subscription.all_consumed_offsets())\n",
    "\n",
    "            # The three main group membership errors are known and should not\n",
    "            # require a stacktrace -- just a warning\n",
    "            except (Errors.UnknownMemberIdError,\n",
    "                    Errors.IllegalGenerationError,\n",
    "                    Errors.RebalanceInProgressError):\n",
    "                log.warning(\"Offset commit failed: group membership out of date\"\n",
    "                            \" This is likely to cause duplicate message\"\n",
    "                            \" delivery.\")\n",
    "            except Exception:\n",
    "                log.exception(\"Offset commit failed: This is likely to cause\"\n",
    "                              \" duplicate message delivery\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-vacation",
   "metadata": {},
   "source": [
    "**如果开启了自动自动位移提交，再均衡时会自动提交当前的消费位移。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-jesus",
   "metadata": {},
   "source": [
    "`kafka.consumer.subscription_state:SubscriptionState` 订阅时注册`ConsumerRebalaceListener()`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-complex",
   "metadata": {},
   "source": [
    "```python\n",
    "class TopicPartitionState(object):\n",
    "    \n",
    "    def subscribe(self, topics=(), pattern=None, listener=None):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-hartford",
   "metadata": {},
   "source": [
    "`kafka.consumer.subscription_state::ConsumerRebalanceListener`实现`on_partitions_revoked()`方法，提交当前位移。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-italy",
   "metadata": {},
   "source": [
    "```python\n",
    "import abc\n",
    "\n",
    "\n",
    "class ConsumerRebalanceListener(object):\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def on_partitions_revoked(self, revoked):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def on_partitions_assigned(self, assigned):\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-importance",
   "metadata": {},
   "source": [
    "## 主题和分区"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-jumping",
   "metadata": {},
   "source": [
    "**从kafka的底层实现来说，主题和分区都是逻辑上的概念，分区可以有一至多个副本，每个副本对应一个日志文件，每个日志文件对应一至多个日志分段（LogSegment），每个日志分段还可以细分为索引文件、日志存储文件和快照文件等。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-tolerance",
   "metadata": {},
   "source": [
    "kafka主题、分区、副本和Log的关系："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-controversy",
   "metadata": {},
   "source": [
    "![kafka主题、分区、副本和Log的关系](https://bj.bcebos.com/ipic/Kafka主题、分区、副本和Log的关系.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-donna",
   "metadata": {},
   "source": [
    "> 主题命名不推荐使用`__`开头命名，因为以`__`开头的主题一般看做kafka内部主题，例如`__consumer_offsets`和`__transaction_state`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-serve",
   "metadata": {},
   "source": [
    "**kafka目前只支持增加分区数而不支持减少分数。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-inclusion",
   "metadata": {},
   "source": [
    "**为什么不支持减少分区？**\n",
    "\n",
    "实现此功能需啊考虑的因素很多，比如删除分区的消息如何处理？如果随着分区一起消失则消息的可靠性得不到保障；如果保留则又需要考虑如何保留。直接存储到现有分区的尾部，消息的时间戳就不会递增。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-possibility",
   "metadata": {},
   "source": [
    "### 分区管理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-capability",
   "metadata": {},
   "source": [
    "分区使用多副本机制提升可靠性，但只有leader副本对外提供服务，而follower副本负责内部消息的同步。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-supplement",
   "metadata": {},
   "source": [
    "**针对同一个分区，同一个broker节点不可能出现它的多个副本。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-howard",
   "metadata": {},
   "source": [
    "生产环境不建议将`auto.leader.rebalance.enable`设置为true，因为这可能引起性能问题，也有可能引起客户端一定时间的阻塞。因为执行时间无法控制，如果在关键时期（电商大促高峰）执行优先副本的自动选举，势必会造成业务阻塞，频繁超时的风险。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-mention",
   "metadata": {},
   "source": [
    "#### 分区重新分配"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-sauce",
   "metadata": {},
   "source": [
    "当集群中新增加节点时，只有新创建的主题才能被分配这个节点，之前创建的主题并不会自动分配新加入的节点中，这样新加入的节点和原来节点的负载严重不均衡。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-vermont",
   "metadata": {},
   "source": [
    "分区重分配的基本原理是先通过控制器为每个分区添加新的副本（增加副本因子），新的副本将从分区的leader副本复制所有的数据，复制完成后，控制器将旧副本从副本清单里移除。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-phase",
   "metadata": {},
   "source": [
    "#### 分区数量越多吞吐量就越高吗"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-vulnerability",
   "metadata": {},
   "source": [
    "分区是kafka中最小的并行操作单元，对于生产者而言，每一个分区的数据写入是完全可以并行化的，对于消费者而言，kafka只允许单个分区的消息被一个消费者消费，消费者组的消费并行度完全依赖于所消费的分区数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-palestinian",
   "metadata": {},
   "source": [
    "**分区数量为1时吞吐量最低，随着分区数的增长，响应的吞吐量也跟着上涨，一旦分区数量超过某个阈值，整体的吞吐量是不升反降的。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-frank",
   "metadata": {},
   "source": [
    "## 日志存储"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-subdivision",
   "metadata": {},
   "source": [
    "一个分区副本对应一个日志文件，为了防止Log过大，kafka又引入日志分段（LogSegment）的概念，将Log切分为多个LogSegment，相当于将巨型文件平均的分配为多个相对较小的文件，这样也便于消息的维护和清理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-regard",
   "metadata": {},
   "source": [
    "![](https://bj.bcebos.com/ipic/Kafka日志关系.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-beatles",
   "metadata": {},
   "source": [
    "一个分区副本对应一个日志文件，为了防止Log过大，kafka又引入日志分段（LogSegment）的概念，将Log切分为多个LogSegment，相当于将巨型文件平均的分配为多个相对较小的文件，这样也便于消息的维护和清理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-mileage",
   "metadata": {},
   "source": [
    "### 日志索引"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-picking",
   "metadata": {},
   "source": [
    "```bash\n",
    "-rw-r--r--  1 hotbaby  wheel    10M Oct 20 17:23 00000000000000000000.index\n",
    "-rw-r--r--  1 hotbaby  wheel     0B Oct 20 17:23 00000000000000000000.log\n",
    "-rw-r--r--  1 hotbaby  wheel    10M Oct 20 17:23 00000000000000000000.timeindex\n",
    "-rw-r--r--  1 hotbaby  wheel     8B Oct 20 17:23 leader-epoch-checkpoint\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-signature",
   "metadata": {},
   "source": [
    "每个日志分段文件对应了两个所以文件，主要是用来提高查找消息的效率。偏移量索引文件用来建立消息偏移量到物理地址的映射关系，方便快速定位消息所在物理文件的位置；时间戳索引文件则根据指定的时间戳（timestamp）来查找对应偏移量消息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-regular",
   "metadata": {},
   "source": [
    "二分查找。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-retail",
   "metadata": {},
   "source": [
    "### 日志清理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-sampling",
   "metadata": {},
   "source": [
    "kafka提供了两种日志清理策略：\n",
    "\n",
    "1. 日志删除（Log Retention）：按照一定的保留策略直接删除不符合条件的日志分段\n",
    "2. 日志压缩（Log Compaction）：针对每个消息的key进行整合，对于有相同key不同value的值，只保留最后一个版本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-alexander",
   "metadata": {},
   "source": [
    "### 磁盘存储"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-cathedral",
   "metadata": {},
   "source": [
    "#### 磁盘I/O流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-arctic",
   "metadata": {},
   "source": [
    "从编程角度而言，一般磁盘I/O的场景分一下4中：\n",
    "\n",
    "1. 用户调用标准C库进行I/O操作，数据流为：应用程序buffer -> C库标准IOBuffer -> 文件系统页缓存 -> 通过具体文件系统到磁盘\n",
    "2. 用户调用文件I/O，数据流为：应用程序buffer -> 文件系统页缓存 -> 通过具体文件系统到磁盘\n",
    "3. 用户打开文件时使用O_DIRECT，绕过页缓存直接读写磁盘\n",
    "4. 用户使用类似`dd`工具，并使用direct参数，绕过系统cache与文件系统直接写磁盘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-concept",
   "metadata": {},
   "source": [
    "## 深入客户端"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-labor",
   "metadata": {},
   "source": [
    "### 消费者协调器和组协调器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-guest",
   "metadata": {},
   "source": [
    "#### 再均衡原理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-farmer",
   "metadata": {},
   "source": [
    "`ConsumerCoordinator`和`GroupCoordinator`之间最重要的职责就是负责执行消费者再均衡的操作。以下几种情况都会触发再均衡操作：\n",
    "\n",
    "* 有新的消费者加入消费者组\n",
    "* 有消费者下线\n",
    "* 有消费者主动退出消费者组\n",
    "* 消费者组对应的`GroupCooridinator`节点变更\n",
    "* 消费组缩影的任意主题分区的数量发生了变化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-whale",
   "metadata": {},
   "source": [
    "再均衡步骤："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-accused",
   "metadata": {},
   "source": [
    "1. FIND_COORDINATOR\n",
    "2. JOIN_GROUP\n",
    "3. SYNC_GROUP\n",
    "4. HEARTBEAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-length",
   "metadata": {},
   "source": [
    "### 事务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-malpractice",
   "metadata": {},
   "source": [
    "#### 消息传输保障"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-district",
   "metadata": {},
   "source": [
    "消息传输保障有3个层级：\n",
    "\n",
    "* `at most once`：至多一次，消息可能会丢失，但绝对不会重复传输\n",
    "* `at least once`：至少一次，消息绝对不会丢失，但可能会重复传输\n",
    "* `exactly once`：恰好一次，消息肯定会被传输，且只传输1次"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-return",
   "metadata": {},
   "source": [
    "kafka从`0.11.0.0`引入了幂等和事务两个特性，以此实现EOS(Exactly Once Sematics)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-willow",
   "metadata": {},
   "source": [
    "#### 幂等"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-control",
   "metadata": {},
   "source": [
    "幂等是对接口的多次调用和一次调用产生的结果是一致的。kafka生产者再进行重试的时候可能会重复写入消息，而使用kafka的幂等性功能后可以避免这种情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-discovery",
   "metadata": {},
   "source": [
    "为了实现生产者的幂等性，kafka引入了producer id（简称PID）和sequence number（序列号）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-beach",
   "metadata": {},
   "source": [
    "每个新的生产者实例在初始化的时候都会分配一个PID，这个PID对用户完全是不可见的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-uzbekistan",
   "metadata": {},
   "source": [
    "broker端会在内存中为每堆<PID, Partition>维护一个序列号SN_old，新消息的序列号SN_new\n",
    "\n",
    "* 新消息序列号SN_new比broker维护的序列号SN_old大于1，broker接收该消息\n",
    "* SN_new小于SN_old+1，说明消息重复写入，该消息被丢弃\n",
    "* SN_new大于SN_old+1，说明中间有消息未写入，出现了乱序，broker会抛出`OutOfOrderSequenceException`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-development",
   "metadata": {},
   "source": [
    "## FAQ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "311.64404296875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
